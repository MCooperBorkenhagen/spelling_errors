---
title: "match to sample"
output: html_document
date: "2023-02-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(tidyverse)
require(readxl)
require(class)
require(RANN)
require(Hmisc)
filter = dplyr::filter

devtools::source_url("https://github.com/MCooperBorkenhagen/utilities/blob/master/eda.R?raw=true")
```



```{r}
set.seed(011)

targets = read_xlsx('data/spelling_error_class_Nancy_person.xlsx') %>% 
  distinct(Spelling_Target) %>% 
  pull(Spelling_Target)

# subset the elp data
elp = read_csv('../words/elp/elp_clean.csv') %>% 
  rownames_to_column() %>% 
  mutate(i = as.numeric(rowname)-1) %>% # account for zero indexing in distance files 
  select(i, word, f = Freq_HAL, length_orth = Length, length_phon = NPhon, length_syll = NSyll, length_morph = NMorph) %>% 
  mutate(target = case_when(word %in% targets ~ TRUE,
                            TRUE ~ FALSE),
         length_phon = as.numeric(length_phon),
         length_syll = as.numeric(length_syll),
         length_morph = as.numeric(length_morph))


targets = elp %>% 
  filter(target) %>%
  select(word, i)

elp_knn = elp %>% 
  select(where(is.numeric)) %>% 
  select(-i) %>% 
  drop_na()

# generate neighbors by kd-tree
neighbors = nn2(elp_knn, query = elp_knn, searchtype = 'priority', k = 25)

```

Let's merge our samples of neighbors from the kd-tree method and clean them up so that they are organized in term of their sample. Notice that the `sample` column is actually the `n` nearest neighbor, where `k` indexes the rank proximity based on the k-tree estimation

```{r}
tmp = elp %>% 
  select(neighbor_index = i, neighbor = word)

nn_i = neighbors$nn.idx %>% 
  as_tibble() %>% 
  rownames_to_column() %>% 
  rename(i = rowname) %>%  
  mutate(i = as.numeric(i)-1) %>%  
  filter(i %in% targets$i) %>% 
  rename_all(~sub('V', '', .x)) %>% 
  pivot_longer(!i, names_to = 'neighbor_rank', values_to = 'neighbor_index') %>% 
  left_join(targets) %>% 
  rename(target = word, target_index = i) %>% 
  left_join(tmp)

nn_dist = neighbors$nn.dists %>% 
  as_tibble() %>%
  rownames_to_column() %>% 
  rename(i = rowname) %>%  
  mutate(i = as.numeric(i)-1) %>%  
  filter(i %in% targets$i) %>% 
  rename_all(~sub('V', '', .x)) %>% 
  pivot_longer(!i, names_to = 'neighbor_rank', values_to = 'neighbor_distance') %>% 
  rename(target_index = i)

samples = nn_i %>% 
  left_join(nn_dist, by = c("target_index", "neighbor_rank")) %>% 
  mutate(neighbor_rank = as.numeric(neighbor_rank)) %>% 
  select(sample = neighbor_rank, target_index, target, neighbor, neighbor_index, neighbor_distance)
  
rm(nn_dist, nn_i, tmp, elp_knn, neighbors)
```

We are using a search type in nearest neighbors that searches away from distance `0` (`searchtype = 'priority'`), so we shouldn't find our target as any of our neighbors. Let's make sure that none of our neighbors is a morphological variant of our target. We didn't control for that.

```{r}
samples %>% 
  mutate(target_first = str_split(target, '', simplify = T)[,1]) %>% 
  mutate(neighbor_first = str_split(neighbor, '', simplify = T)[,1]) %>%
  filter(target_first == neighbor_first) %>% 
  View()
  

```



```{r}
removes = c('aqueducts', 'charlatans', 'bureaucrat', 'embarrassing', 'miscellanies', 'poignantly', 'separated', 'tranquility', 'tranquilizer')

samples = samples %>% 
  filter(neighbor %nin% removes) %>% 
  group_by(target) %>% 
  arrange(-desc(sample)) %>% 
  mutate(rank = seq_len(n())) %>% 
  select(-sample) %>% 
  filter(rank < 21) %>%
  select(sample = rank, everything()) %>% 
  glimpse()

# write samples to file:
samples %>% 
  write_csv('data/samples.csv')
```


Now we need to subset our pairwise distances. We need to be sure that all the words from all our samples are present when we subset our distances (for orth, phon, sem). Note that this portion of the script takes a long time to run because loading these giant datafiles is intensive.

```{r}
D = read_csv('data/all_orth_distances.csv', col_names = T)

D %>% 
  filter(index1 %in% unique(samples$target_index, samples$neighbor_index) | index2 %in% unique(samples$target_index, samples$neighbor_index)) %>%
  write_csv('data/subset_orth_distances.csv')

so = read_csv('data/subset_orth_distances.csv')

rm(D)

D = read_csv('data/all_phone_distances.csv', col_names = T)


D %>% 
  filter(index1 %in% unique(samples$target_index, samples$neighbor_index) | index2 %in% unique(samples$target_index, samples$neighbor_index)) %>%
  write_csv('data/subset_phon_distances.csv')


```

